{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generating_jida_prose",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnHyoEZlfDc4",
        "colab_type": "text"
      },
      "source": [
        "জীবনানন্দ দাশের মতোন কবিতা লেখার চেষ্টা করা যাক!\n",
        "\n",
        "জীবনানন্দের সমস্ত কবিতা [এইখান](https://www.bangla-kobita.com/jibanananda/) থেকে স্ক্র্যাপ করে র ডেটাসেট তৈরি করছি। "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ehpByNfz4KD",
        "colab_type": "text"
      },
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzPkSV-RgPxq",
        "colab_type": "text"
      },
      "source": [
        "ড্রাইভ থেকে ডেটাসেট ডাউনলোড করলাম।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkLNELOAuWtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://gdurl.com/s26P/download\n",
        "!mv download jida.txt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn4JbI9mgUyP",
        "colab_type": "text"
      },
      "source": [
        "এইবার প্রিপ্রসেসিং। টেক্স ডেটা নিয়া কাজ করবার সবচাইতে গুরুত্বপূর্ণ কাজ ঝাড়ু দেয়া।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8-QOm0Iu1BT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('jida.txt') as f:\n",
        "  raw = f.read()\n",
        "  \n",
        "raw = [r for r in raw.split('\\n') if 'কাব্যগ্রন্থ' not in r]\n",
        "raw = ' '.join(raw)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdkBaycvqLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw = raw.translate(str.maketrans('', '', '–!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~“”\\u200c—।১২৩৪৫৬৭৮৯০\\xa0'))\n",
        "raw = raw.replace(\"\\'\", \"’\")\n",
        "tokens = raw.split()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmpZqdFng4nE",
        "colab_type": "text"
      },
      "source": [
        "এমনভাবে লাইন-শব্দগুলোকে সাজানো হলো যাতে বিশটা শব্দ ট্রেইনিংয়ে গিয়ে শেষ শব্দ প্রেডিক্ট করা যায়। ডেটাসেটের কয়েক লাইন দেখলে স্পষ্ট হয়ে যাবে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzK1_zNezDgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = 20 + 1\n",
        "sequences = []\n",
        "for i in range(seq_length, len(tokens)):\n",
        "  seq = tokens[i-seq_length:i]\n",
        "  line = ' '.join(seq)\n",
        "  sequences.append(line)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnp84VMZgyOy",
        "colab_type": "text"
      },
      "source": [
        "ঝাড়ু দিয়ে আবর্জনা দূর করে নতুন ডেটাসেট তৈরি করলাম।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V41-gumzzXcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('jida_processed.txt', 'w') as f:\n",
        "  doc = '\\n'.join(sequences)\n",
        "  f.write(doc)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZVouyoGz6in",
        "colab_type": "text"
      },
      "source": [
        "### Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv8WvJ3WhkOx",
        "colab_type": "text"
      },
      "source": [
        "প্রসেস করা ডেটা পেয়ে গেলে এবার গৎবাধা কাজ। মডেল তৈরি করে সেখানে ডেটা ফিট করা। আর্কিটেকচারে যেসব লেয়ার নেয়া হলো- তার বিস্তারিত লেখাপড়া দরকার। নইলে ক্যানো-কিভাবে কাজ করতেছে- বোঝা যাবে না।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5Mzx5ia02lM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array\n",
        "from pickle import dump\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNB5TNuj09JG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('jida_processed.txt') as f:\n",
        "  doc = f.read()\n",
        "lines = doc.split('\\n')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_727VkDuztX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRxusxbQjK8E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fb8fcc45-08c2-4e6c-a323-d4adc949a754"
      },
      "source": [
        "len(sequences), len(sequences[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(71038, 21)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33bs6WLe3rhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIPdWULr1cbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = array(sequences)\n",
        "X, y = sequences[:, :-1], sequences[:, -1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC6o3VqqHhin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.21)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QckxEIjSJBmm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "30502da9-ff11-4815-a024-199a46cdab6b"
      },
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape, "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((56120, 20), (14918, 20), (56120, 14393), (14918, 14393))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKye6x5VMpnv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "08058b6b-4b69-4bd7-ceb8-31ad3518c36f"
      },
      "source": [
        "# modified\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(Bidirectional(LSTM(170, return_sequences = True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(300, activation='relu'))\n",
        "model.add(Dense(600, activation='relu'))\n",
        "model.add(Dense(1200, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 20, 50)            719650    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 20, 340)           300560    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 20, 340)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 150)               294600    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 300)               45300     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 600)               180600    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1200)              721200    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 14393)             17285993  \n",
            "=================================================================\n",
            "Total params: 19,547,903\n",
            "Trainable params: 19,547,903\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itHBFu3iAvYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "earlyStopping = EarlyStopping(monitor='loss', patience=21, verbose=0, mode='min')\n",
        "model_save = ModelCheckpoint('model_light.h5', save_best_only=True, monitor='loss', mode='min')\n",
        "reduce_lr_loss = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h57wjw9xEu2O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d504f43-d3e0-46e6-ab4b-0c4d65723583"
      },
      "source": [
        "history = model.fit(X, y, batch_size=200, epochs=500, verbose=1,\n",
        "                    callbacks=[earlyStopping, model_save, reduce_lr_loss])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 8.3107 - accuracy: 0.0080 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 7.9721 - accuracy: 0.0102 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 7.8090 - accuracy: 0.0107 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 7.6539 - accuracy: 0.0124 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 7.4679 - accuracy: 0.0155 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 7.2386 - accuracy: 0.0205 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 6.9793 - accuracy: 0.0270 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 6.7020 - accuracy: 0.0345 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 6.4008 - accuracy: 0.0423 - lr: 0.0010\n",
            "Epoch 10/500\n",
            "356/356 [==============================] - 15s 41ms/step - loss: 6.0759 - accuracy: 0.0509 - lr: 0.0010\n",
            "Epoch 11/500\n",
            "356/356 [==============================] - 15s 41ms/step - loss: 5.7383 - accuracy: 0.0632 - lr: 0.0010\n",
            "Epoch 12/500\n",
            "356/356 [==============================] - 15s 42ms/step - loss: 5.3861 - accuracy: 0.0765 - lr: 0.0010\n",
            "Epoch 13/500\n",
            "356/356 [==============================] - 15s 41ms/step - loss: 5.0309 - accuracy: 0.0919 - lr: 0.0010\n",
            "Epoch 14/500\n",
            "356/356 [==============================] - 15s 41ms/step - loss: 4.6985 - accuracy: 0.1107 - lr: 0.0010\n",
            "Epoch 15/500\n",
            "356/356 [==============================] - 14s 41ms/step - loss: 4.3770 - accuracy: 0.1325 - lr: 0.0010\n",
            "Epoch 16/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 4.0725 - accuracy: 0.1608 - lr: 0.0010\n",
            "Epoch 17/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 3.7883 - accuracy: 0.1930 - lr: 0.0010\n",
            "Epoch 18/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 3.5197 - accuracy: 0.2282 - lr: 0.0010\n",
            "Epoch 19/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 3.2653 - accuracy: 0.2657 - lr: 0.0010\n",
            "Epoch 20/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 3.0335 - accuracy: 0.3001 - lr: 0.0010\n",
            "Epoch 21/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 2.8280 - accuracy: 0.3343 - lr: 0.0010\n",
            "Epoch 22/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 2.6235 - accuracy: 0.3707 - lr: 0.0010\n",
            "Epoch 23/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 2.4505 - accuracy: 0.4031 - lr: 0.0010\n",
            "Epoch 24/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 2.2803 - accuracy: 0.4354 - lr: 0.0010\n",
            "Epoch 25/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 2.1226 - accuracy: 0.4656 - lr: 0.0010\n",
            "Epoch 26/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 1.9929 - accuracy: 0.4923 - lr: 0.0010\n",
            "Epoch 27/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 1.8692 - accuracy: 0.5191 - lr: 0.0010\n",
            "Epoch 28/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 1.7558 - accuracy: 0.5443 - lr: 0.0010\n",
            "Epoch 29/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 1.6594 - accuracy: 0.5645 - lr: 0.0010\n",
            "Epoch 30/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 1.5631 - accuracy: 0.5844 - lr: 0.0010\n",
            "Epoch 31/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 1.4633 - accuracy: 0.6076 - lr: 0.0010\n",
            "Epoch 32/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 1.4064 - accuracy: 0.6205 - lr: 0.0010\n",
            "Epoch 33/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 1.3283 - accuracy: 0.6383 - lr: 0.0010\n",
            "Epoch 34/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 1.2525 - accuracy: 0.6576 - lr: 0.0010\n",
            "Epoch 35/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 1.1893 - accuracy: 0.6727 - lr: 0.0010\n",
            "Epoch 36/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 1.1291 - accuracy: 0.6853 - lr: 0.0010\n",
            "Epoch 37/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 1.0753 - accuracy: 0.6989 - lr: 0.0010\n",
            "Epoch 38/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 1.0273 - accuracy: 0.7116 - lr: 0.0010\n",
            "Epoch 39/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.9781 - accuracy: 0.7256 - lr: 0.0010\n",
            "Epoch 40/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.9368 - accuracy: 0.7345 - lr: 0.0010\n",
            "Epoch 41/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.8999 - accuracy: 0.7416 - lr: 0.0010\n",
            "Epoch 42/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 0.8647 - accuracy: 0.7519 - lr: 0.0010\n",
            "Epoch 43/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.8139 - accuracy: 0.7660 - lr: 0.0010\n",
            "Epoch 44/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.7987 - accuracy: 0.7688 - lr: 0.0010\n",
            "Epoch 45/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.7615 - accuracy: 0.7794 - lr: 0.0010\n",
            "Epoch 46/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.7211 - accuracy: 0.7923 - lr: 0.0010\n",
            "Epoch 47/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.6851 - accuracy: 0.8019 - lr: 0.0010\n",
            "Epoch 48/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.6776 - accuracy: 0.8028 - lr: 0.0010\n",
            "Epoch 49/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.6409 - accuracy: 0.8124 - lr: 0.0010\n",
            "Epoch 50/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.6388 - accuracy: 0.8126 - lr: 0.0010\n",
            "Epoch 51/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.5899 - accuracy: 0.8262 - lr: 0.0010\n",
            "Epoch 52/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.5863 - accuracy: 0.8288 - lr: 0.0010\n",
            "Epoch 53/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 0.5620 - accuracy: 0.8331 - lr: 0.0010\n",
            "Epoch 54/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.5423 - accuracy: 0.8397 - lr: 0.0010\n",
            "Epoch 55/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.5254 - accuracy: 0.8432 - lr: 0.0010\n",
            "Epoch 56/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 0.5038 - accuracy: 0.8508 - lr: 0.0010\n",
            "Epoch 57/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.5033 - accuracy: 0.8512 - lr: 0.0010\n",
            "Epoch 58/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.4792 - accuracy: 0.8584 - lr: 0.0010\n",
            "Epoch 59/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 0.4497 - accuracy: 0.8666 - lr: 0.0010\n",
            "Epoch 60/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.4594 - accuracy: 0.8637 - lr: 0.0010\n",
            "Epoch 61/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.4344 - accuracy: 0.8701 - lr: 0.0010\n",
            "Epoch 62/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.4348 - accuracy: 0.8706 - lr: 0.0010\n",
            "Epoch 63/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.4117 - accuracy: 0.8775 - lr: 0.0010\n",
            "Epoch 64/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 0.3992 - accuracy: 0.8810 - lr: 0.0010\n",
            "Epoch 65/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.3946 - accuracy: 0.8822 - lr: 0.0010\n",
            "Epoch 66/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.3889 - accuracy: 0.8835 - lr: 0.0010\n",
            "Epoch 67/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.3833 - accuracy: 0.8855 - lr: 0.0010\n",
            "Epoch 68/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.3476 - accuracy: 0.8946 - lr: 0.0010\n",
            "Epoch 69/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.3472 - accuracy: 0.8964 - lr: 0.0010\n",
            "Epoch 70/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.3424 - accuracy: 0.8978 - lr: 0.0010\n",
            "Epoch 71/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.3365 - accuracy: 0.8996 - lr: 0.0010\n",
            "Epoch 72/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.3291 - accuracy: 0.9026 - lr: 0.0010\n",
            "Epoch 73/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.3040 - accuracy: 0.9091 - lr: 0.0010\n",
            "Epoch 74/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.3166 - accuracy: 0.9064 - lr: 0.0010\n",
            "Epoch 75/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.3181 - accuracy: 0.9050 - lr: 0.0010\n",
            "Epoch 76/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.3138 - accuracy: 0.9066 - lr: 0.0010\n",
            "Epoch 77/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.2812 - accuracy: 0.9158 - lr: 0.0010\n",
            "Epoch 78/500\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 0.2819 - accuracy: 0.9159 - lr: 0.0010\n",
            "Epoch 79/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.2799 - accuracy: 0.9159 - lr: 0.0010\n",
            "Epoch 80/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.2889 - accuracy: 0.9139 - lr: 0.0010\n",
            "Epoch 81/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.2866 - accuracy: 0.9146 - lr: 0.0010\n",
            "Epoch 82/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.2614 - accuracy: 0.9220 - lr: 0.0010\n",
            "Epoch 83/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.2648 - accuracy: 0.9218 - lr: 0.0010\n",
            "Epoch 84/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.2482 - accuracy: 0.9262 - lr: 0.0010\n",
            "Epoch 85/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.2668 - accuracy: 0.9211 - lr: 0.0010\n",
            "Epoch 86/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 0.2403 - accuracy: 0.9282 - lr: 0.0010\n",
            "Epoch 87/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.2524 - accuracy: 0.9258 - lr: 0.0010\n",
            "Epoch 88/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.2491 - accuracy: 0.9254 - lr: 0.0010\n",
            "Epoch 89/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.2430 - accuracy: 0.9268 - lr: 0.0010\n",
            "Epoch 90/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.2299 - accuracy: 0.9308 - lr: 0.0010\n",
            "Epoch 91/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.2134 - accuracy: 0.9369 - lr: 0.0010\n",
            "Epoch 92/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.2337 - accuracy: 0.9309 - lr: 0.0010\n",
            "Epoch 93/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.2271 - accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 94/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.2307 - accuracy: 0.9321 - lr: 0.0010\n",
            "Epoch 95/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.2161 - accuracy: 0.9356 - lr: 0.0010\n",
            "Epoch 96/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.2106 - accuracy: 0.9360 - lr: 0.0010\n",
            "Epoch 97/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.2211 - accuracy: 0.9350 - lr: 0.0010\n",
            "Epoch 98/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.2069 - accuracy: 0.9388 - lr: 0.0010\n",
            "Epoch 99/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.2011 - accuracy: 0.9411 - lr: 0.0010\n",
            "Epoch 100/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.1960 - accuracy: 0.9414 - lr: 0.0010\n",
            "Epoch 101/500\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 0.2005 - accuracy: 0.9406 - lr: 0.0010\n",
            "Epoch 102/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.1821 - accuracy: 0.9462 - lr: 0.0010\n",
            "Epoch 103/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1963 - accuracy: 0.9419 - lr: 0.0010\n",
            "Epoch 104/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.2067 - accuracy: 0.9378 - lr: 0.0010\n",
            "Epoch 105/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.2054 - accuracy: 0.9386 - lr: 0.0010\n",
            "Epoch 106/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.1798 - accuracy: 0.9461 - lr: 0.0010\n",
            "Epoch 107/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.1661 - accuracy: 0.9518 - lr: 0.0010\n",
            "Epoch 108/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1772 - accuracy: 0.9486 - lr: 0.0010\n",
            "Epoch 109/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1936 - accuracy: 0.9421 - lr: 0.0010\n",
            "Epoch 110/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1812 - accuracy: 0.9464 - lr: 0.0010\n",
            "Epoch 111/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1690 - accuracy: 0.9490 - lr: 0.0010\n",
            "Epoch 112/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.1641 - accuracy: 0.9511 - lr: 0.0010\n",
            "Epoch 113/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1736 - accuracy: 0.9487 - lr: 0.0010\n",
            "Epoch 114/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1790 - accuracy: 0.9461 - lr: 0.0010\n",
            "Epoch 115/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1674 - accuracy: 0.9503 - lr: 0.0010\n",
            "Epoch 116/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1748 - accuracy: 0.9484 - lr: 0.0010\n",
            "Epoch 117/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.1517 - accuracy: 0.9549 - lr: 0.0010\n",
            "Epoch 118/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1591 - accuracy: 0.9532 - lr: 0.0010\n",
            "Epoch 119/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1602 - accuracy: 0.9530 - lr: 0.0010\n",
            "Epoch 120/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1564 - accuracy: 0.9541 - lr: 0.0010\n",
            "Epoch 121/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1603 - accuracy: 0.9533 - lr: 0.0010\n",
            "Epoch 122/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1559 - accuracy: 0.9546 - lr: 0.0010\n",
            "Epoch 123/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1532 - accuracy: 0.9546 - lr: 0.0010\n",
            "Epoch 124/500\n",
            "356/356 [==============================] - 14s 40ms/step - loss: 0.1439 - accuracy: 0.9575 - lr: 0.0010\n",
            "Epoch 125/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1557 - accuracy: 0.9543 - lr: 0.0010\n",
            "Epoch 126/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1593 - accuracy: 0.9527 - lr: 0.0010\n",
            "Epoch 127/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1465 - accuracy: 0.9570 - lr: 0.0010\n",
            "Epoch 128/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.1321 - accuracy: 0.9600 - lr: 0.0010\n",
            "Epoch 129/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1417 - accuracy: 0.9577 - lr: 0.0010\n",
            "Epoch 130/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1590 - accuracy: 0.9533 - lr: 0.0010\n",
            "Epoch 131/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1359 - accuracy: 0.9603 - lr: 0.0010\n",
            "Epoch 132/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.1314 - accuracy: 0.9609 - lr: 0.0010\n",
            "Epoch 133/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1407 - accuracy: 0.9584 - lr: 0.0010\n",
            "Epoch 134/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1362 - accuracy: 0.9590 - lr: 0.0010\n",
            "Epoch 135/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1392 - accuracy: 0.9583 - lr: 0.0010\n",
            "Epoch 136/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1437 - accuracy: 0.9577 - lr: 0.0010\n",
            "Epoch 137/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1400 - accuracy: 0.9585 - lr: 0.0010\n",
            "Epoch 138/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.1253 - accuracy: 0.9620 - lr: 0.0010\n",
            "Epoch 139/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.1234 - accuracy: 0.9624 - lr: 0.0010\n",
            "Epoch 140/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 0.1253 - accuracy: 0.9622 - lr: 0.0010\n",
            "Epoch 141/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1243 - accuracy: 0.9625 - lr: 0.0010\n",
            "Epoch 142/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.1218 - accuracy: 0.9639 - lr: 0.0010\n",
            "Epoch 143/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 0.1254 - accuracy: 0.9626 - lr: 0.0010\n",
            "Epoch 144/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1303 - accuracy: 0.9613 - lr: 0.0010\n",
            "Epoch 145/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1270 - accuracy: 0.9616 - lr: 0.0010\n",
            "Epoch 146/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 0.1268 - accuracy: 0.9625 - lr: 0.0010\n",
            "Epoch 147/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.1195 - accuracy: 0.9636 - lr: 0.0010\n",
            "Epoch 148/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.1162 - accuracy: 0.9669 - lr: 0.0010\n",
            "Epoch 149/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1214 - accuracy: 0.9643 - lr: 0.0010\n",
            "Epoch 150/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 0.1225 - accuracy: 0.9638 - lr: 0.0010\n",
            "Epoch 151/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.1157 - accuracy: 0.9656 - lr: 0.0010\n",
            "Epoch 152/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 0.1160 - accuracy: 0.9651 - lr: 0.0010\n",
            "Epoch 153/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1159 - accuracy: 0.9656 - lr: 0.0010\n",
            "Epoch 154/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 0.1184 - accuracy: 0.9645 - lr: 0.0010\n",
            "Epoch 155/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.1117 - accuracy: 0.9663 - lr: 0.0010\n",
            "Epoch 156/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1215 - accuracy: 0.9637 - lr: 0.0010\n",
            "Epoch 157/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1211 - accuracy: 0.9646 - lr: 0.0010\n",
            "Epoch 158/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.1099 - accuracy: 0.9673 - lr: 0.0010\n",
            "Epoch 159/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.1085 - accuracy: 0.9676 - lr: 0.0010\n",
            "Epoch 160/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0996 - accuracy: 0.9700 - lr: 0.0010\n",
            "Epoch 161/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 0.1089 - accuracy: 0.9683 - lr: 0.0010\n",
            "Epoch 162/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 0.1090 - accuracy: 0.9683 - lr: 0.0010\n",
            "Epoch 163/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1017 - accuracy: 0.9702 - lr: 0.0010\n",
            "Epoch 164/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1072 - accuracy: 0.9683 - lr: 0.0010\n",
            "Epoch 165/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1052 - accuracy: 0.9692 - lr: 0.0010\n",
            "Epoch 166/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0988 - accuracy: 0.9706 - lr: 0.0010\n",
            "Epoch 167/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1033 - accuracy: 0.9695 - lr: 0.0010\n",
            "Epoch 168/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1128 - accuracy: 0.9668 - lr: 0.0010\n",
            "Epoch 169/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0924 - accuracy: 0.9724 - lr: 0.0010\n",
            "Epoch 170/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1068 - accuracy: 0.9685 - lr: 0.0010\n",
            "Epoch 171/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1092 - accuracy: 0.9689 - lr: 0.0010\n",
            "Epoch 172/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1000 - accuracy: 0.9703 - lr: 0.0010\n",
            "Epoch 173/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0907 - accuracy: 0.9727 - lr: 0.0010\n",
            "Epoch 174/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 0.0969 - accuracy: 0.9711 - lr: 0.0010\n",
            "Epoch 175/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.0976 - accuracy: 0.9715 - lr: 0.0010\n",
            "Epoch 176/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.0928 - accuracy: 0.9723 - lr: 0.0010\n",
            "Epoch 177/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.0939 - accuracy: 0.9720 - lr: 0.0010\n",
            "Epoch 178/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.0956 - accuracy: 0.9716 - lr: 0.0010\n",
            "Epoch 179/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.0966 - accuracy: 0.9711 - lr: 0.0010\n",
            "Epoch 180/500\n",
            "356/356 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9679\n",
            "Epoch 00180: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 0.1087 - accuracy: 0.9679 - lr: 0.0010\n",
            "Epoch 181/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0493 - accuracy: 0.9853 - lr: 1.0000e-04\n",
            "Epoch 182/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0182 - accuracy: 0.9950 - lr: 1.0000e-04\n",
            "Epoch 183/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0111 - accuracy: 0.9974 - lr: 1.0000e-04\n",
            "Epoch 184/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0084 - accuracy: 0.9979 - lr: 1.0000e-04\n",
            "Epoch 185/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0063 - accuracy: 0.9984 - lr: 1.0000e-04\n",
            "Epoch 186/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0054 - accuracy: 0.9987 - lr: 1.0000e-04\n",
            "Epoch 187/500\n",
            "356/356 [==============================] - 14s 38ms/step - loss: 0.0044 - accuracy: 0.9990 - lr: 1.0000e-04\n",
            "Epoch 188/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0039 - accuracy: 0.9991 - lr: 1.0000e-04\n",
            "Epoch 189/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0038 - accuracy: 0.9990 - lr: 1.0000e-04\n",
            "Epoch 190/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0032 - accuracy: 0.9991 - lr: 1.0000e-04\n",
            "Epoch 191/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0031 - accuracy: 0.9991 - lr: 1.0000e-04\n",
            "Epoch 192/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0030 - accuracy: 0.9993 - lr: 1.0000e-04\n",
            "Epoch 193/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0028 - accuracy: 0.9991 - lr: 1.0000e-04\n",
            "Epoch 194/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0023 - accuracy: 0.9993 - lr: 1.0000e-04\n",
            "Epoch 195/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 0.0024 - accuracy: 0.9993 - lr: 1.0000e-04\n",
            "Epoch 196/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 0.0023 - accuracy: 0.9992 - lr: 1.0000e-04\n",
            "Epoch 197/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0020 - accuracy: 0.9993 - lr: 1.0000e-04\n",
            "Epoch 198/500\n",
            "356/356 [==============================] - 14s 38ms/step - loss: 0.0020 - accuracy: 0.9993 - lr: 1.0000e-04\n",
            "Epoch 199/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 0.0021 - accuracy: 0.9993 - lr: 1.0000e-04\n",
            "Epoch 200/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 0.0021 - accuracy: 0.9994 - lr: 1.0000e-04\n",
            "Epoch 201/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0017 - accuracy: 0.9994 - lr: 1.0000e-04\n",
            "Epoch 202/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0017 - accuracy: 0.9994 - lr: 1.0000e-04\n",
            "Epoch 203/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 0.0018 - accuracy: 0.9993 - lr: 1.0000e-04\n",
            "Epoch 204/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 0.0020 - accuracy: 0.9992 - lr: 1.0000e-04\n",
            "Epoch 205/500\n",
            "356/356 [==============================] - 14s 38ms/step - loss: 0.0016 - accuracy: 0.9993 - lr: 1.0000e-04\n",
            "Epoch 206/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 0.0017 - accuracy: 0.9993 - lr: 1.0000e-04\n",
            "Epoch 207/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 0.0019 - accuracy: 0.9993 - lr: 1.0000e-04\n",
            "Epoch 208/500\n",
            "355/356 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9994\n",
            "Epoch 00208: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 0.0018 - accuracy: 0.9994 - lr: 1.0000e-04\n",
            "Epoch 209/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0013 - accuracy: 0.9995 - lr: 1.0000e-05\n",
            "Epoch 210/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 0.0012 - accuracy: 0.9996 - lr: 1.0000e-05\n",
            "Epoch 211/500\n",
            "356/356 [==============================] - 14s 38ms/step - loss: 0.0011 - accuracy: 0.9996 - lr: 1.0000e-05\n",
            "Epoch 212/500\n",
            "356/356 [==============================] - 14s 38ms/step - loss: 0.0011 - accuracy: 0.9996 - lr: 1.0000e-05\n",
            "Epoch 213/500\n",
            "356/356 [==============================] - 14s 38ms/step - loss: 9.3802e-04 - accuracy: 0.9996 - lr: 1.0000e-05\n",
            "Epoch 214/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 0.0011 - accuracy: 0.9995 - lr: 1.0000e-05\n",
            "Epoch 215/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 8.7188e-04 - accuracy: 0.9996 - lr: 1.0000e-05\n",
            "Epoch 216/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 8.6438e-04 - accuracy: 0.9997 - lr: 1.0000e-05\n",
            "Epoch 217/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 8.5300e-04 - accuracy: 0.9996 - lr: 1.0000e-05\n",
            "Epoch 218/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 8.8522e-04 - accuracy: 0.9996 - lr: 1.0000e-05\n",
            "Epoch 219/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 8.6816e-04 - accuracy: 0.9996 - lr: 1.0000e-05\n",
            "Epoch 220/500\n",
            "356/356 [==============================] - 14s 38ms/step - loss: 7.8861e-04 - accuracy: 0.9997 - lr: 1.0000e-05\n",
            "Epoch 221/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 8.4557e-04 - accuracy: 0.9996 - lr: 1.0000e-05\n",
            "Epoch 222/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 8.1864e-04 - accuracy: 0.9996 - lr: 1.0000e-05\n",
            "Epoch 223/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 8.1335e-04 - accuracy: 0.9995 - lr: 1.0000e-05\n",
            "Epoch 224/500\n",
            "356/356 [==============================] - 14s 38ms/step - loss: 7.6011e-04 - accuracy: 0.9997 - lr: 1.0000e-05\n",
            "Epoch 225/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 8.0709e-04 - accuracy: 0.9996 - lr: 1.0000e-05\n",
            "Epoch 226/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.8359e-04 - accuracy: 0.9997 - lr: 1.0000e-05\n",
            "Epoch 227/500\n",
            "355/356 [============================>.] - ETA: 0s - loss: 7.7153e-04 - accuracy: 0.9996\n",
            "Epoch 00227: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.7113e-04 - accuracy: 0.9996 - lr: 1.0000e-05\n",
            "Epoch 228/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.6650e-04 - accuracy: 0.9997 - lr: 1.0000e-06\n",
            "Epoch 229/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 7.3859e-04 - accuracy: 0.9996 - lr: 1.0000e-06\n",
            "Epoch 230/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 7.2692e-04 - accuracy: 0.9997 - lr: 1.0000e-06\n",
            "Epoch 231/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.3800e-04 - accuracy: 0.9997 - lr: 1.0000e-06\n",
            "Epoch 232/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 7.6316e-04 - accuracy: 0.9996 - lr: 1.0000e-06\n",
            "Epoch 233/500\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 7.0784e-04 - accuracy: 0.9997 - lr: 1.0000e-06\n",
            "Epoch 234/500\n",
            "355/356 [============================>.] - ETA: 0s - loss: 7.3144e-04 - accuracy: 0.9997\n",
            "Epoch 00234: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.3106e-04 - accuracy: 0.9997 - lr: 1.0000e-06\n",
            "Epoch 235/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 8.1287e-04 - accuracy: 0.9997 - lr: 1.0000e-07\n",
            "Epoch 236/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.6338e-04 - accuracy: 0.9996 - lr: 1.0000e-07\n",
            "Epoch 237/500\n",
            "356/356 [==============================] - 14s 38ms/step - loss: 6.9239e-04 - accuracy: 0.9997 - lr: 1.0000e-07\n",
            "Epoch 238/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.5387e-04 - accuracy: 0.9996 - lr: 1.0000e-07\n",
            "Epoch 239/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.2890e-04 - accuracy: 0.9996 - lr: 1.0000e-07\n",
            "Epoch 240/500\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 7.1630e-04 - accuracy: 0.9996 - lr: 1.0000e-07\n",
            "Epoch 241/500\n",
            "355/356 [============================>.] - ETA: 0s - loss: 7.1459e-04 - accuracy: 0.9997\n",
            "Epoch 00241: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.1436e-04 - accuracy: 0.9997 - lr: 1.0000e-07\n",
            "Epoch 242/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.0020e-04 - accuracy: 0.9996 - lr: 1.0000e-08\n",
            "Epoch 243/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.0263e-04 - accuracy: 0.9997 - lr: 1.0000e-08\n",
            "Epoch 244/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.1691e-04 - accuracy: 0.9997 - lr: 1.0000e-08\n",
            "Epoch 245/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.6398e-04 - accuracy: 0.9996 - lr: 1.0000e-08\n",
            "Epoch 246/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.1332e-04 - accuracy: 0.9997 - lr: 1.0000e-08\n",
            "Epoch 247/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.1293e-04 - accuracy: 0.9997 - lr: 1.0000e-08\n",
            "Epoch 248/500\n",
            "355/356 [============================>.] - ETA: 0s - loss: 7.5322e-04 - accuracy: 0.9997\n",
            "Epoch 00248: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
            "356/356 [==============================] - 13s 35ms/step - loss: 7.5287e-04 - accuracy: 0.9997 - lr: 1.0000e-08\n",
            "Epoch 249/500\n",
            "356/356 [==============================] - 13s 35ms/step - loss: 7.7023e-04 - accuracy: 0.9997 - lr: 1.0000e-09\n",
            "Epoch 250/500\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 6.5226e-04 - accuracy: 0.9998 - lr: 1.0000e-09\n",
            "Epoch 251/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.2818e-04 - accuracy: 0.9996 - lr: 1.0000e-09\n",
            "Epoch 252/500\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 6.5154e-04 - accuracy: 0.9997 - lr: 1.0000e-09\n",
            "Epoch 253/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.1656e-04 - accuracy: 0.9997 - lr: 1.0000e-09\n",
            "Epoch 254/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 6.7904e-04 - accuracy: 0.9997 - lr: 1.0000e-09\n",
            "Epoch 255/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 8.1093e-04 - accuracy: 0.9996 - lr: 1.0000e-09\n",
            "Epoch 256/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.8409e-04 - accuracy: 0.9996 - lr: 1.0000e-09\n",
            "Epoch 257/500\n",
            "355/356 [============================>.] - ETA: 0s - loss: 6.9781e-04 - accuracy: 0.9997\n",
            "Epoch 00257: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 6.9766e-04 - accuracy: 0.9997 - lr: 1.0000e-09\n",
            "Epoch 258/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.1806e-04 - accuracy: 0.9996 - lr: 1.0000e-10\n",
            "Epoch 259/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.8327e-04 - accuracy: 0.9996 - lr: 1.0000e-10\n",
            "Epoch 260/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.0505e-04 - accuracy: 0.9997 - lr: 1.0000e-10\n",
            "Epoch 261/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.3835e-04 - accuracy: 0.9996 - lr: 1.0000e-10\n",
            "Epoch 262/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 6.9308e-04 - accuracy: 0.9997 - lr: 1.0000e-10\n",
            "Epoch 263/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.4339e-04 - accuracy: 0.9996 - lr: 1.0000e-10\n",
            "Epoch 264/500\n",
            "355/356 [============================>.] - ETA: 0s - loss: 7.7461e-04 - accuracy: 0.9996\n",
            "Epoch 00264: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.7421e-04 - accuracy: 0.9996 - lr: 1.0000e-10\n",
            "Epoch 265/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.9978e-04 - accuracy: 0.9996 - lr: 1.0000e-11\n",
            "Epoch 266/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.7900e-04 - accuracy: 0.9996 - lr: 1.0000e-11\n",
            "Epoch 267/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.2683e-04 - accuracy: 0.9997 - lr: 1.0000e-11\n",
            "Epoch 268/500\n",
            "356/356 [==============================] - 13s 35ms/step - loss: 6.8847e-04 - accuracy: 0.9997 - lr: 1.0000e-11\n",
            "Epoch 269/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.2660e-04 - accuracy: 0.9997 - lr: 1.0000e-11\n",
            "Epoch 270/500\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 7.6531e-04 - accuracy: 0.9996 - lr: 1.0000e-11\n",
            "Epoch 271/500\n",
            "355/356 [============================>.] - ETA: 0s - loss: 6.6426e-04 - accuracy: 0.9998\n",
            "Epoch 00271: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
            "356/356 [==============================] - 13s 36ms/step - loss: 6.6394e-04 - accuracy: 0.9998 - lr: 1.0000e-11\n",
            "Epoch 272/500\n",
            "356/356 [==============================] - 13s 35ms/step - loss: 7.6857e-04 - accuracy: 0.9997 - lr: 1.0000e-12\n",
            "Epoch 273/500\n",
            "356/356 [==============================] - 13s 35ms/step - loss: 7.1256e-04 - accuracy: 0.9997 - lr: 1.0000e-12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVGzk8VzjqVv",
        "colab_type": "text"
      },
      "source": [
        "### Generating Prose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn-XCwBLh39Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "45b909c0-3b7b-4af4-9717-b6864d26c000"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVvW1KJojELT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the tokenizer\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlRZxw2Qh5ar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp model_light.h5 '/content/drive/My Drive/Colab Notebooks/'\n",
        "!cp tokenizer.pkl '/content/drive/My Drive/Colab Notebooks/'"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjUoLv2xoXla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import randint\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO_5X6-5jS8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('jida_processed.txt') as f:\n",
        "  doc = f.read()\n",
        "lines = doc.split('\\n')\n",
        "seq_length = len(lines[0].split()) - 1"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FBWdzyIxT--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to download model_ligth.h5 and tokenizer.pkl (if you don't want to train)\n",
        "# !wget -qq https://drive.google.com/u/0/uc?id=1MqmaI_vQysOOQ1IyKuxWGNXpvC8adAYZ&export=download\n",
        "# !mv \"/content/uc?id=1MqmaI_vQysOOQ1IyKuxWGNXpvC8adAYZ\" model_light.h5\n",
        "# !wget -qq https://drive.google.com/u/0/uc?id=1OeqZF59Oop0IJO-MJB0SOvjwACdHNbZp&export=download\n",
        "# !mv \"/content/uc?id=1OeqZF59Oop0IJO-MJB0SOvjwACdHNbZp\" tokenizer.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtlWUNV0jvAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from pickle import load\n",
        "\n",
        "model = load_model('model_light.h5')\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olCK8opRkh5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate a sequence from a language model\n",
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "\tresult = list()\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# map predicted word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += ' ' + out_word\n",
        "\t\tresult.append(out_word)\n",
        "\treturn ' '.join(result)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKJfWpFGvgap",
        "colab_type": "text"
      },
      "source": [
        "প্রথম কয়েকটা শব্দ ধরায় দিলে দিব্বি জীদার ভাষায় কবিতা বের হতে থাকবে এখন।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SukYUy8uMvo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f4e7aa96-7559-4419-dacc-25d89d42100e"
      },
      "source": [
        "seed_text = lines[randint(0, len(lines))]\n",
        "seed_text"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'কাজ কিবা হায় বিয়ে হয়েছিল কবেমরে গেছে বউ যদিও মহুয়া গাছে ফুটে ওঠে মৌ একবার ঝরে গেলে তবু তারপর মহুয়া মহুয়া'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_LifERmvuv9",
        "colab_type": "text"
      },
      "source": [
        "প্রতিবার পাঁচশো শব্দ উৎপাদন হচ্ছে- দশ শব্দে প্রতি লাইন পদ্যের ভাব আনতে। নইলে গদ্যও পড়া চলে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doF12O0tnVCd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "outputId": "8265a5d0-b68d-4d38-c0be-8c19be8ddd00"
      },
      "source": [
        "seed_text = 'গ্রীষ্মের সমুদ্র থেকে চোখের ঘুমের গান আসিতেছে ভেসে'\n",
        "generated = generate_seq(model, tokenizer, seq_length, seed_text, 500)\n",
        "\n",
        "i = 0\n",
        "for w in generated.split():\n",
        "  print(w, end=' ')\n",
        "  i += 1\n",
        "  if not i % 10:\n",
        "    print()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "এখানে পালঙ্কে শুয়ে কাটিবে অনেক দিন জেগে থেকে ঘুমাবার সাধ \n",
            "ভালোবেসে তবুও যখন মৃত্যু হবে উপস্থিত আরএকটি প্রভাতের হয়তো বা \n",
            "অন্যতর বিস্তীর্ণতায় মনে হবে অনেক প্রতীক্ষা মোরা করে গেছি পৃথিবীতে \n",
            "চোয়ালের মাংস ক্রমে ক্ষীণ করে কোনো এক বিশীর্ণ কাকের অক্ষিগোলকের \n",
            "সাথে আঁখিতারকার সব সমাহার এক দেখে তবু লঘু হাস্যে সন্তানের \n",
            "জন্ম দিয়ে তারা আমাদের মতো হবে সেই কথা জেনে ভুলে \n",
            "গিয়ে লোল হাস্যে জলের তরঙ্গ মোরা শুনে গেছি আমাদের প্রাণের \n",
            "ভিতর নব শিকড়ের স্বাদ অনুভব করে গেছি ভোরের স্ফটিক রৌদ্রে \n",
            "অনেক গন্ধর্ব নাগ কুকুর কিন্নর পঙ্গপাল বহুবিধ জন্তুর কপাল উন্মোচিত \n",
            "হয়ে বিরুদ্ধে দাঁড়ায়ে থাকে পথ পথান্তরে তবু ওই নীলিমাকে প্রিয় \n",
            "অভিভাবিকার মতো মনে হয় হাতে তার তুলাদণ্ড শান্ত স্থির মুখের \n",
            "প্রতিজ্ঞাপাশে নির্জন নীলাভ বৃত্তি ছাড়া কিছু নেই যেন তার কাছে \n",
            "জীবনের অভ্যুদয় মধ্য সমুদ্রের পরে অনুকূল বাতাসের প্ররোচনাময় কোনো এক \n",
            "ক্রীড়া ক্রীড়া বেরিলমণির মতো তরঙ্গের উজ্জ্বল আঘাতে মৃত্যু স্থির শুভ্র \n",
            "নৈসর্গিক কথা বলিবার অবসর অশ্বত্থ বটের পথে অনেক হয়েছি আমি \n",
            "তোমাদের সাথী ছড়ায়েছি খই ধান বহুদিন উঠানের শালিখের তরে সন্ধ্যায় \n",
            "পুকুর থেকে হাঁসটিরে নিয়ে আমি তোমাদের ঘরে গিয়েছি অনেক দিন \n",
            "দেখিয়াছি ধূপ জ্বালো ধরো সন্ধ্যাবাতি থোড়ের মতন শাদা ভিজে হাতে \n",
            "এখুনি আসিবে কিনা রাতি বিনুনি বেঁধেছ তাইকাঁচাপোকাটিপ তুমি কপালের পরে \n",
            "পড়িয়াছতারপর ঘুমায়েছঃ কল্কাপাড় আঁচলটি ঝরে পানের বাটার পরে নোনার মতো \n",
            "নম্র শরীরটি পাতি নির্জন পালঙ্কে তুমি ঘুমায়েছ বউকথাকওটির ছানা নীল \n",
            "জামরুল নীড়ে জ্যোৎস্নায় ঘুমায়ে রয়েছে যেন হায় আর রাত্রি মাতাপাখাটির \n",
            "মতো ছড়ায়ে রয়েছে তার ডানা আজ আমি ক্লান্ত চোখে ব্যবহৃত \n",
            "জীবনের ধূলোয় কাঁটায় চলে গেছি বহুদূরেদ্যাখোনিকো বোঝোনিকো করনিকো মানা রূপসী \n",
            "শঙ্খের কৌটা তুমি যে গো প্রাণহীন পানের বাটায় অশ্বত্থে সন্ধ্যার \n",
            "হাওয়া যখন লেগেছে নীল বাংলার বনে মাঠে মাঠে ফিরি একা \n",
            "মনে হয় বাংলার জীবনে সঙ্কট শেষ হয়ে গেছে আজ চেয়ে \n",
            "দেখ কতো শত শতাব্দীর বট হাজার সবুজ পাতা লাল ফল \n",
            "বুকে লয়ে শাখার ব্যজনে আকাঙ্খার গান গায় অশ্বত্থেরও কি যেন \n",
            "কামনা জাগে মনে সতীর শীতল শব বহু দিন কোলে লয়ে \n",
            "যেন অকপট উমার প্রেমের গল্প পেয়েছে সে চন্দ্রশেখরের মতো তার \n",
            "জট উজ্জ্বল হতেছে তাই সপ্তমীর চাঁদের আজ পুনরাগমনে মধুকূপী ঘাসছাওয়া \n",
            "ধলেশ্বরীটির পাড়ে গৌরী বাংলার এবার বল্লাল সেন আসিবে না জানি \n",
            "আমি রায়গুণাকর আসিবে না দেশবন্ধু আসিয়াছে ক্ষুরধার পদ্মায় এবার কালীগহে \n",
            "ক্লান্ত গাঙশালিখের ভিড়ে যেন আসিয়াছে ঝড় আসিয়াছে চন্ডীদাস রামপ্রসাদের শ্যামা \n",
            "সাথে সাথে তার শঙ্খমালা চন্দ্রমালা মৃত শত কিশোরীর কঙ্কণের স্বর \n",
            "ভালোবাসিয়াছি আমি অস্তচাঁদ ক্লান্ত শেষপ্রহরের শশী অঘোর ঘুমের ঘোরে ঢলে \n",
            "যবে কালো নদীঢেউয়ের কলসী নিঝ্ঝুম বিছানার পরে মেঘবৌর খোঁপাখসা জোছনাফুল \n",
            "চুপে চুপে ঝরে চেয়ে থাকি চোখ তুলেযেন মোর পলাতকা প্রিয়া \n",
            "মেঘের ঘোমটা তুলে প্রেতচাঁদে সচকিতে ওঠে শিহরিয়া সে যেন দেখেছে \n",
            "মোরে জন্মে জন্মে ফিরে ফিরে ফিরে মাঠে ঘাটে একা একা \n",
            "বুনোহাঁসজোনাকির ভিড়ে দুশ্চর দেউলে কোন্কোন্ যক্ষপ্রাসাদের তটে দূর উরব্যাবিলোনমিশরের মরুভূসঙ্কটে \n",
            "কোথা পিরামিড তলে ঈসিসের বেদিকার মূলে কেউটের মতো নীলা যেইখানে \n",
            "ফণা তুলে উঠিয়াছে ফুলে কোন্ মনভুলানিয়া পথচাওয়া দুলালীর মনে আমারে \n",
            "দেখেছে জোছনাচোর চোখেঅলস নয়নে আমারে দেখেছে সে যে আসরীয় সম্রাটের \n",
            "বেশে প্রাসাদঅলিন্দে যবে মহিমায় দাঁড়ায়েছি এসে হাতে তার হাত পায়ে \n",
            "হাতিয়ার রাখি কুমারীর পানে আমি তুলিয়াছি আনন্দের আরক্তিম আঁখি ভোরগেলাসের \n",
            "সুরাতহুরা করেছি মোরা চুপে চুপে পান চকোরজুড়ির মতো কুহরিয়া গাহিয়াছি \n",
            "চাঁদিনীর গান পেয়ালায়পায়েলায় সেই নিশি হয় নি উতলা নীল নিচোলের \n",
            "কোলে নাচে নাই আকাশের তলা নটীরা ঘুমায়েছিল পুরে পুরে ঘুমের \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxvDrOzxued9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "outputId": "8cc1e037-496b-4aa3-ac31-5697d8f7a7f3"
      },
      "source": [
        "seed_text = 'মরিবার হলো তার সাধ'\n",
        "generated = generate_seq(model, tokenizer, seq_length, seed_text, 500)\n",
        "\n",
        "i = 0\n",
        "for w in generated.split():\n",
        "  print(w, end=' ')\n",
        "  i += 1\n",
        "  if not i % 10:\n",
        "    print()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "মাসবেহুলার লহনার মধুর জগতে আজ একদিন ঘরে দুই চোখ ঠোট \n",
            "নাসিকা আঙুল তাহার ছোয়াচে ভিজে গেছে চুল শাদা শাদা ফেনাফুলে \n",
            "কত বার দূর উপকূলে তারাভরা আকাশের তলে বালকের মতো এক \n",
            "সমুদ্রের জলে দেহ ধুয়ে নিয়া জেনেছি দেহের স্বাদ গেছে বুক \n",
            "মুখ পরশিয়া রাঙা রোদ নারীর মতন এ দেহ পেয়েছে যেন \n",
            "তাহার চুম্বন ফসলের ক্ষেতে প্রথম প্রণয়ী সে যে কার্তিকের ভোরবেলা \n",
            "দূরে যেতে যেতে থেমে গেছে সে আমার তরে চোখ দুটো \n",
            "ফের ঘুমে ভরে যেন তার চুমো খেয়ে এ দেহ অলস \n",
            "মেয়ে পুরুষের সোহাগে অবশ চুমে লয় রৌদ্রের রস হেমন্ত বৈকালে \n",
            "উড়ো পাখাপাখালির পালে উঠানের পেতে থাকে কান শোনো ঝরা শিশিরের \n",
            "গান অঘ্রানের মাঝরাতে হিম হাওয়া যেন শাদা কঙ্কালের হাতে এ \n",
            "দেহেরে এসে ধরে ব্যথা দেয় নারীর অধরে চুলে চোখে জুঁয়ের \n",
            "নিশ্বাসে ঝুমকো লতার মতো তার দেহ ফাঁসে ভরা ফসলের মতো \n",
            "পড়ে ছিঁড়ে এই দেহ ব্যথা পায় ফিরে… তবু এই শস্যক্ষেতে \n",
            "পিপাসার ভাষা ফুরাবে না কে বা সেই চাষা কাস্তে হাতে \n",
            "কঠিন কামুক আমাদের সবটুকু ব্যথাভরা সুখ উচ্ছেদ করিবে এসে একা \n",
            "কে বা সেই জানি না তো হয় নাই দেখা আজও \n",
            "তার সনে আজ শুধু দেহ আর দেহের পীড়নে সাধ মোর \n",
            "চোখে ঠোঁটে চুলে শুধু পীড়া শুধু পীড়া মুকুলে মুকুলে শুধু \n",
            "কীট আঘাত দংশন চায় আজ মন নক্ষত্রের পানে যেতে যেতে \n",
            "পথ ভুলে বারবার পৃথিবীর ক্ষেতে জন্মিতেছি আমি এক সবুজ ফসল \n",
            "অন্ধকারে শিশিরের জল কানে কানে গাহিয়াছে গান ঢালিয়াছে শীতল অঘ্রাণ \n",
            "মোর দেহ ছেনে গেছে অলস আঢুল কুমারী আঙুল কুয়াশার ঘ্রাণ \n",
            "আর পরশের সাধ জাগায়েছে কাস্তের মতো বাঁকা চাঁদ ঢালিয়াছে আলো \n",
            "প্রণয়ীর ঠোঁটের ধারালো চুম্বনের মতো রেখে গেছে ক্ষত সব্জির সবুজ \n",
            "রুধিরে শস্যের মতো মোর এ শরীর ছিঁড়ে বারবার হয়েছে আহত \n",
            "আগুনের মতো দুপুরের রাঙা রোদ আমি তবু ব্যথা দেই ব্যথা \n",
            "পাই ফিরে তবু চাই সবুজ শরীরে এ ব্যথার সুখ লাল \n",
            "আলো রৌদ্রের চুমুক অন্ধকার কুয়াশার ছুরি মোরে যেন কেটে লয় \n",
            "যেন গুড়ি গুড়ি ধুলো মোরে ধীরে লয় শুষে মাঠে মাঠে \n",
            "আড়ষ্ট পউষে ফসলের গন্ধ বুকে করে বারবার পড়ি যেন ঝ’রে \n",
            "আবার পাব আমি ফিরে এই দেহ এ মাটির নিঃসাড় শিশিরে \n",
            "রক্তের তাপ ঢেলে আমি আসিব কি নামি হেমন্তের রৌদ্রের মতন \n",
            "ফসলের স্তন আঙুলে নিঙাড়ি এক ক্ষেত ছাড়ি অন্য ক্ষেতে চলিব \n",
            "কি ভেসে এ সবুজ দেশে আর এক বার শুনিব কি \n",
            "গান ঢেউদের জলের আঘ্রাণ লব বুকে তুলে আমি পথ ভুলে \n",
            "আসিব কি এ পথে আবার ধুলো বিছানার কীটদের মতো হব \n",
            "কি আহত ঘাসের আঘাতে বেদনার সাথে সুখ পাব লতার মতন \n",
            "মোর চুল আমার আঙুল পাপড়ির মতো হবে কি বিক্ষত তোমার \n",
            "আঙুলে চুলে লাগিবে কি ফুলে ফুলের আঘাত আরবার আমার এ \n",
            "পিপাসার ধার তোমাদের জাগাবে পিপাসা ক্ষুধিতের ভাষা বুকে করে করে \n",
            "ফলিব কি পড়িব কি ঝরে পৃথিবীর শস্যের ক্ষেতে আর একবার \n",
            "আমি নক্ষত্রের পানে যেতে যেতে বেলা বয়ে যায় গোধূলির মেঘসীমানায় \n",
            "ধূম্র মৌন সাঁঝে নিত্য নব দিবসের মৃতু্যঘন্টা বাজে শতাব্দীর শবদেহে \n",
            "শ্মশানের ভষ্মবহ্নি জ্বলে পান্থ ম্লান চিতার কবলে একে একে ডুবে \n",
            "যায় দেশ জাতিসংসার সমাজ কার লাগি হে সমাধি তুমি একা \n",
            "বসে আছ আজ কী এক বিক্ষুব্ধ প্রেতকায়ার মতন অতীতের শোভাযাত্রা \n",
            "কোথায় কখন চকিতে মিলায়ে গেছেপাও নাই টের কোন দিবা অবসানে \n",
            "গৌরবের লক্ষ মুসাফের দেউটি নিভায়ে গেছেচলে গেছে দেউল ত্যজিয়া চলে \n",
            "গেছে প্রিয়তমচলে গেছে প্রিয়া যুগান্ত্মের মণিময় গেহবাস ছাড়ি চকিতে চলিয়া \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}